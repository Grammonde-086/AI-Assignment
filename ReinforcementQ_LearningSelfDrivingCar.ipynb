{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Overview:\n",
    "The Smartcab's job is to pick up the passenger at one location and drop them off in another. Here are a few things that we'd love our Smartcab to take care of:\n",
    "<ul>\n",
    "<li>Drop off the passenger to the right location.</li>\n",
    "<li>Save passenger's time by taking minimum time possible to drop off</li>\n",
    "<li>Take care of passenger's safety and traffic rules</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Components that need to be considered for designing our agent:\n",
    "<ul>\n",
    "    <li>Reward:\n",
    "        <ul>\n",
    "            <li>Our agent is reward-motivated and is going to learn how to control the cab by trial experiences in the environment</li>\n",
    "            <li>The agent should receive a high positive reward for a successful dropoff because this behavior is highly desired</li>\n",
    "            <li>The agent should be penalized if it tries to drop off a passenger in wrong locations\n",
    "        The agent should get a slight negative reward for not making it to the destination after every time-step. \"Slight\" negative because we would prefer our agent to reach late instead of making wrong moves trying to reach to the destination as fast as possible</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>State Space:\n",
    "        <ul>\n",
    "            <li>Agents may encounter a state, and then take action according to the state it's in.</li>\n",
    "            <li>State should contain useful information the agent needs to make the right action.</li>\n",
    "            <li>PEAS for our car agent:\n",
    "                <ul>\n",
    "                    <li>Performance: try to get high positive reward</li>\n",
    "                    <li>Environment:\n",
    "                        <ul>\n",
    "                            <li>5x5 grid, give 25 possible position \n",
    "                            <li>There are 4 position R,G,B,Y that is the pick up or drop off position\n",
    "                            <li>assume max passenger in the car (3 passenger) which mean there are 5*5*5*4=500 possible position\n",
    "                        </ul>\n",
    "                    </li>\n",
    "                    <li>Actuator: Wheel</li>\n",
    "                    <li>Sensor: Camera or some computer vision</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Action Space:\n",
    "        <ul>\n",
    "            <li>set of all the actions that our agent can take in a given state.</li>\n",
    "            <li>Can be move north, south, west, east, pick up and drop off</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Framework that use for this project is : OpenAI Gym\n",
    "<ul>\n",
    "    <li>provides different game environments which we can plug into our code and test an agent.</li>\n",
    "    <li>providing all the information that our agent would require, like possible actions, score, and current state.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gym\n",
      "  Using cached gym-0.17.1.tar.gz (1.6 MB)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\envs\\myenv\\lib\\site-packages (from gym) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.10.4 in c:\\programdata\\anaconda3\\envs\\myenv\\lib\\site-packages (from gym) (1.18.1)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\envs\\myenv\\lib\\site-packages (from gym) (1.14.0)\n",
      "Collecting pyglet<=1.5.0,>=1.4.0\n",
      "  Using cached pyglet-1.5.0-py2.py3-none-any.whl (1.0 MB)\n",
      "Collecting cloudpickle<1.4.0,>=1.2.0\n",
      "  Using cached cloudpickle-1.3.0-py2.py3-none-any.whl (26 kB)\n",
      "Collecting future\n",
      "  Using cached future-0.18.2.tar.gz (829 kB)\n",
      "Building wheels for collected packages: gym, future\n",
      "  Building wheel for gym (setup.py): started\n",
      "  Building wheel for gym (setup.py): finished with status 'done'\n",
      "  Created wheel for gym: filename=gym-0.17.1-py3-none-any.whl size=1648717 sha256=10acb8a293b3f5b1aae3ea96710a69f53ac37a75c76d8abdc9e3fbc1654e2b56\n",
      "  Stored in directory: c:\\users\\administrator\\appdata\\local\\pip\\cache\\wheels\\12\\7a\\2a\\2e85bca5dd2c3b319675a5db8a48837b7cfe0603240442b771\n",
      "  Building wheel for future (setup.py): started\n",
      "  Building wheel for future (setup.py): finished with status 'done'\n",
      "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491062 sha256=3e343eb96aa82cd312716a9abdb3ccc96902926eb9387a45a9116b97fa71704e\n",
      "  Stored in directory: c:\\users\\administrator\\appdata\\local\\pip\\cache\\wheels\\56\\b0\\fe\\4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
      "Successfully built gym future\n",
      "Installing collected packages: future, pyglet, cloudpickle, gym\n",
      "Successfully installed cloudpickle-1.3.0 future-0.18.2 gym-0.17.1 pyglet-1.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: 253\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : :\u001b[43m \u001b[0m: : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"Taxi-v3\").env\n",
    "state = env.encode(2, 2, 3, 1) # (taxi row, taxi column, passenger index, destination index)\n",
    "print(\"State:\", state)\n",
    "\n",
    "env.s = state\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our Taxi-V2 environment:\n",
    "<ul>\n",
    "    <li>The filled square represents the taxi, which is yellow without a passenger and green with a passenger.</li>\n",
    "    <li>The pipe (\"|\") represents a wall which the taxi cannot cross.</li>\n",
    "    <li>R, G, Y, B are the possible pickup and destination locations. The blue letter represents the current passenger pick-up location, and the purple letter is the current destination.</li>\n",
    "    <li>To identify a state uniquely by assigning a unique number to every possible state like:\n",
    "        <ul>\n",
    "            <li>0 = south</li>\n",
    "            <li>1 = north</li>\n",
    "            <li>2 = east</li>\n",
    "            <li>3 = west</li>\n",
    "            <li>4 = pickup</li>\n",
    "            <li>5 = dropoff</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space Discrete(6)\n",
      "State Space Discrete(500)\n"
     ]
    }
   ],
   "source": [
    "print(\"Action Space {}\".format(env.action_space))\n",
    "print(\"State Space {}\".format(env.observation_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement Learning will learn a mapping of states to the optimal action to perform in that state by exploration, i.e. the agent explores the environment and takes actions based off rewards defined in the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our Reward Table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(1.0, 353, -1, False)],\n",
       " 1: [(1.0, 153, -1, False)],\n",
       " 2: [(1.0, 273, -1, False)],\n",
       " 3: [(1.0, 233, -1, False)],\n",
       " 4: [(1.0, 253, -10, False)],\n",
       " 5: [(1.0, 253, -10, False)]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.P[253]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>number of states as rows and number of actions as columns.</li>\n",
    "    <li>The above Dictionary: <code>{action: [(probability, nextstate, reward, done)]}</code>\n",
    "        <ul>\n",
    "            <li><code>actions</code> (south, north, east, west, pickup, dropoff) the taxi can perform at our current state</li>\n",
    "            <li><code>probability</code> is always 1.0.</li>\n",
    "            <li>All the movement actions have a -1 <code>reward</code> and the pickup/dropoff actions have -10 reward in this particular state. We would see a reward of 20 at the dropoff action when it drop off on the right destination</li>\n",
    "            <li><code>done</code> is used to tell us when we have successfully dropped off a passenger in the right location.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without Reinforcement Learning:\n",
    "<ul>\n",
    "    <li>Navigate taxi based on initial reward table</li>\n",
    "    <li>Create an infinite loop which runs until one passenger reaches one destination</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timesteps taken: 2177\n",
      "Penalties incurred: 705\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Without Q-Learning\"\"\"\n",
    "env.s = 253  # set environment to illustration's state\n",
    "\n",
    "epochs = 0\n",
    "penalties, reward = 0, 0\n",
    "\n",
    "frames = [] # for animation\n",
    "\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action)\n",
    "\n",
    "    if reward == -10:\n",
    "        penalties += 1\n",
    "    \n",
    "    # Put each rendered frame into dict for animation\n",
    "    frames.append({\n",
    "        'frame': env.render(mode='ansi'),\n",
    "        'state': state,\n",
    "        'action': action,\n",
    "        'reward': reward\n",
    "        }\n",
    "    )\n",
    "\n",
    "    epochs += 1\n",
    "    \n",
    "    \n",
    "print(\"Timesteps taken: {}\".format(epochs))\n",
    "print(\"Penalties incurred: {}\".format(penalties))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[35m\u001b[34;1m\u001b[43mG\u001b[0m\u001b[0m\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "\n",
      "Timestep: 2177\n",
      "State: 85\n",
      "Action: 5\n",
      "Reward: 20\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "def print_frames(frames):\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'])\n",
    "        print(f\"Timestep: {i + 1}\")\n",
    "        print(f\"State: {frame['state']}\")\n",
    "        print(f\"Action: {frame['action']}\")\n",
    "        print(f\"Reward: {frame['reward']}\")\n",
    "        sleep(.1)\n",
    "        \n",
    "print_frames(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not good. Our agent takes thousand timesteps and makes lots of wrong drop offs to deliver just one passenger to the right destination.\n",
    "\n",
    "This is because we aren't learning from past experience. We can run this over and over, and it will never optimize. The agent has no memory of which action was best for each state, which is exactly what Reinforcement Learning will do for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning\n",
    "Q-learning lets the agent use the environment's rewards to learn, over time, the best action to take in a given state.\n",
    "It does thing by looking receiving a reward for taking an action in the current state, then updating a Q-value to remember if that action was beneficial.\n",
    "Q-values map to a <code>(state, action)</code> combination and it is the representative of the \"quality\" of an action taken from that state. Better Q-values imply better chances of getting greater rewards.<br>\n",
    "Q-values are updated using the equation:\n",
    "<code>Q(state,action)←(1−α)Q(state,action)+α(reward+γmaxaQ(next state,all actions))</code>\n",
    "<ul>\n",
    "    <li><code>α</code>(alpha) is the learning rate. It is the extent to which our Q-values are being updated in every iteration.</li>\n",
    "    <li><code>γ</code>(gamma) is the discount factor. It determines how much importance we want to give to future rewards.</li>\n",
    "</ul>\n",
    "From the the above equation: We are assigning (←), or updating, the Q-value of the agent's current state and action by first taking a weight (1−α) of the old Q-value, then adding the learned value. The learned value is a combination of the reward for taking the current action in the current state, and the discounted maximum reward from the next state we will be in once we take the current action.\n",
    "\n",
    "#### Q-Table\n",
    "The Q-table is a matrix where we have a row for every state (500) and a column for every action (6). It's first initialized to 0, and then values are updated after training.\n",
    "\n",
    "#### Q-Learning Process:\n",
    "Breaking it down into steps, we get\n",
    "<ul>\n",
    "    <li>Initialize the Q-table by all zeros.</li>\n",
    "    <li>Start exploring actions: For each state, select any one among all possible actions for the current state (S).</li>\n",
    "    <li>Travel to the next state (S') as a result of that action (a).</li>\n",
    "    <li>For all possible actions from the state (S') select the one with the highest Q-value.</li>\n",
    "    <li>Update Q-table values using the equation.</li>\n",
    "    <li>Set the next state as the current state.</li>\n",
    "    <li>If goal state is reached, then end and repeat the process.</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Agent\n",
    "Initialize the Q-table to a 500×6 matrix of zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start Training Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100000\n",
      "Training finished.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Training the agent\"\"\"\n",
    "\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1\n",
    "\n",
    "# For plotting metrics\n",
    "all_epochs = []\n",
    "all_penalties = []\n",
    "\n",
    "for i in range(1, 100001):\n",
    "    state = env.reset()\n",
    "\n",
    "    epochs, penalties, reward, = 0, 0, 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample() # Explore action space\n",
    "        else:\n",
    "            action = np.argmax(q_table[state]) # Exploit learned values\n",
    "\n",
    "        next_state, reward, done, info = env.step(action) \n",
    "        \n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        \n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        q_table[state, action] = new_value\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        state = next_state\n",
    "        epochs += 1\n",
    "        \n",
    "    if i % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Episode: {i}\")\n",
    "\n",
    "print(\"Training finished.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first part of <code>while not done</code>, we decide whether to pick a random action or to exploit the already computed Q-values. This is done simply by using the <code>epsilon</code> value and comparing it to the <code>random.uniform(0, 1)</code> function, which returns an arbitrary number between 0 and 1.\n",
    "\n",
    "We execute the chosen action in the environment to obtain the <code>next_state</code> and the <code>reward</code> from performing the action. After that, we calculate the maximum Q-value for the actions corresponding to the <code>next_state</code>, and with that, we can easily update our Q-value to the <code>new_q_value</code>:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-table has been established over 100,000 episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -2.41836112,  -2.41834853,  -2.27325184,  -2.41836473,\n",
       "       -11.36355604, -11.36242973])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Q-table has been established over 100,000 episodes\"\"\"\n",
    "q_table[253]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The max Q-value is \"east\" (-2.27) and based from the above figure it is the best action to be taken from the current state. Seem like our agent is learnt from the Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results after 100 episodes:\n",
      "Average timesteps per episode: 13.14\n",
      "Average penalties per episode: 0.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Evaluate agent's performance after Q-learning\"\"\"\n",
    "\n",
    "total_epochs, total_penalties = 0, 0\n",
    "episodes = 100\n",
    "\n",
    "for _ in range(episodes):\n",
    "    state = env.reset()\n",
    "    epochs, penalties, reward = 0, 0, 0\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = np.argmax(q_table[state])\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        epochs += 1\n",
    "\n",
    "    total_penalties += penalties\n",
    "    total_epochs += epochs\n",
    "\n",
    "print(f\"Results after {episodes} episodes:\")\n",
    "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
    "print(f\"Average penalties per episode: {total_penalties / episodes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the evaluation, the agent's performance improved significantly and it incurred no penalties, which means it performed the correct pickup/dropoff actions with 100 different passengers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
